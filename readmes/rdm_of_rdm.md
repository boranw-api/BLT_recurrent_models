# Joint Representation RDM Structure (RDM of RDMs)

This document explains the **RDM of RDMs** visualization generated by the `rnn_test.py` script. This figure creates a unified matrix visualizing the meta-similarity of the model's internal representations across all layers and time steps simultaneously.

## 1. What is this figure?

This is a **Joint Representation RDM Structure Matrix** (often called a "Second-Order RDM" or "RDM of RDMs").

Instead of showing *what* the model sees (e.g., "this image looks like a dog"), this figure shows **how the geometry of the Representation space changes** as information flows through the network layers and evolves over recurrent time steps.

## 2. Methodology

The visualization is built using a two-step RDM calculation process:

1.  **Feature Extraction**:
    *   We run the model on a fixed set of test images (e.g., 1000 images).
    *   For every single combination of **Layer** (e.g., Output 0, Output 1...) and **Time Step** ($t=0, t=1, t=2...$), we record the activation vectors for all images.

2.  **First-Order RDM Calculation**:
    *   For each specific `(Layer, Time)` state, we compute a **Representational Dissimilarity Matrix (RDM)**.
    *   An RDM is a square matrix (Image x Image) where each value represents the distance between two images in that specific high-dimensional feature space. This captures the "geometric fingerprint" of that layer-time state.

3.  **Second-Order RDM Calculation (RDM Comparison)**:
    *   We then treat each flattened RDM from Step 2 as a single data point.
    *   We calculate the similarity (using Cosine Similarity by default) between **every pair of RDMs**.
    *   The resulting large matrix (Total Steps x Total Steps) tells us: *"How similar is the representational geometry at (Layer A, Time T1) to the geometry at (Layer B, Time T2)?"*

## 3. How to Interpret

The figure visualizes relationships using a heatmap matrix:

### A. Axes
*   Both X and Y axes represent the sequence of all layers laid out in order of time. The labels indicate the Layer blocks.
*   **White Grid Lines**: These lines mark the boundaries between different Layers (e.g., separating Output 0 from Output 1).
*   **Inside a Block**: Within each grid block, the axis progresses through time steps ($t_0 \to t_n$).

### B. Color Scale
*   **Blue (Cool colors)**: Indicates **High Similarity** (Low Distance). The geometries are nearly identical.
*   **Red (Warm colors)**: Indicates **High Dissimilarity** (High Distance). The geometries are very different.

### C. Key Patterns to Look For

| Pattern | Interpretation |
| :--- | :--- |
| **The Diagonal** | Always dark blue because every state is identical to itself. |
| **Blue Block Diagonal** | Representation is **stable across time** within that layer (recurrence isn't changing the geometry much). |
| **Gradient Block Diagonal** | Representation is **evolving dynamically** over time (blue fading to red). |
| **Blue Off-Diagonal** | Smooth transformation between layers; geometry is preserved. |
| **Red Off-Diagonal** | Radical transformation between layers; geometry changes significantly. |

Use this explanation when presenting the figure to collaborators to clarify that we are analyzing the **stability and evolution of the representational manifolds** themselves, not just individual neuron activations.